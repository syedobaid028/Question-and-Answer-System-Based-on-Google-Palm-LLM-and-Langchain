# -*- coding: utf-8 -*-
"""LLM project using langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZhrzVEQWtB-pxZFmQJCGPAG3Ti-PaIBU
"""

!pip install langchain==0.0.284
!pip install python-dotenv==1.0.0
!pip install streamlit==1.22.0
!pip install tiktoken==0.4.0
!pip install faiss-cpu==1.7.4
!pip install protobuf~=3.19.0

from langchain.llms import GooglePalm
api_key="AIzaSyAUj0FuPT-EtrS8Rn_LotjNQU7QmxE4mPE"
llm=GooglePalm(google_api_key=api_key,temperature=0.7)

poem=llm("write me a poem for the love of burgers")

print(poem)

email=llm("write me a email for getting raise")
print(email)

from langchain.chains import RetrievalQA


from langchain.embeddings import GooglePalmEmbeddings
from langchain.llms import GooglePalm

from langchain.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(file_path='codebasics_faqs.csv', source_column="prompt", encoding='latin-1')
data = loader.load()

pip install -U sentence-transformers==2.2.2

from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.vectorstores import FAISS

instructor_embeddings = HuggingFaceInstructEmbeddings()
vectordb=FAISS.from_documents(documents=data,embedding=instructor_embeddings)

retriever=vectordb.as_retriever()
rdocs=retriever.get_relevant_documents("how long this course gonna last?")
rdocs

from langchain.prompts import PromptTemplate

prompt_template = """Given the following context and a question, generate an answer based on this context only.
In the answer try to provide as much text as possible from "response" section in the source document context without making much changes.
If the answer is not found in the context, kindly state "I don't know." Don't try to make up an answer.

CONTEXT: {context}

QUESTION: {question}"""


PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
chain_type_kwargs = {"prompt": PROMPT}


from langchain.chains import RetrievalQA

chain = RetrievalQA.from_chain_type(llm=llm,
                            chain_type="stuff",
                            retriever=retriever,
                            input_key="query",
                            return_source_documents=True,
                            chain_type_kwargs=chain_type_kwargs)

chain("do you have a job opening?")

